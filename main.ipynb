{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74eebd51",
   "metadata": {},
   "source": [
    "# Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c2c43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import docx\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad8f02",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "additional_stopwords = {\"а\",\n",
    "\"без\",\n",
    "\"безо\",\n",
    "\"в\",\n",
    "\"во\",\n",
    "\"для\",\n",
    "\"до\",\n",
    "\"за\",\n",
    "\"из\",\n",
    "\"изо\",\n",
    "\"к\",\n",
    "\"ко\",\n",
    "\"на\",\n",
    "\"над\",\n",
    "\"надо\",\n",
    "\"о\",\n",
    "\"об\",\n",
    "\"обо\",\n",
    "\"от\",\n",
    "\"ото\",\n",
    "\"по\",\n",
    "\"под\",\n",
    "\"подо\",\n",
    "\"при\",\n",
    "\"про\",\n",
    "\"с\",\n",
    "\"со\",\n",
    "\"у\",\n",
    "\"через\",\n",
    "\"чрез\",\n",
    "\"и\",\n",
    "\"тоже\",\n",
    "\"также\",\n",
    "\"но\",\n",
    "\"однако\",\n",
    "\"зато\",\n",
    "\"же\",\n",
    "\"или\",\n",
    "\"либо\",\n",
    "\"то\",\n",
    "\"что\",\n",
    "\"чтобы\",\n",
    "\"как\",\n",
    "\"будто\",\n",
    "\"когда\",\n",
    "\"пока\",\n",
    "\"едва\",\n",
    "\"потому\",\n",
    "\"так\",\n",
    "\"ибо\",\n",
    "\"оттого\",\n",
    "\"чтобы\",\n",
    "\"если\",\n",
    "\"бы\",\n",
    "\"раз\",\n",
    "\"коли\",\n",
    "\"хотя\",\n",
    "\"хоть\",\n",
    "\"пускай\",\n",
    "\"как\",\n",
    "\"будто\",\n",
    "\"словно\",\n",
    "\"точно\",\n",
    "\"давай\",\n",
    "\"давайте\",\n",
    "\"пусть\",\n",
    "\"пускай\",\n",
    "\"бы\",\n",
    "\"б\",\n",
    "\"же\",\n",
    "\"даже\",\n",
    "\"именно\",\n",
    "\"только\",\n",
    "\"лишь\",\n",
    "\"хоть\",\n",
    "\"исключительно\",\n",
    "\"единственно\",\n",
    "\"просто\",\n",
    "\"прямо\",\n",
    "\"вот\",\n",
    "\"вон\",\n",
    "\"это\",\n",
    "\"ли\",\n",
    "\"ль\",\n",
    "\"разве\",\n",
    "\"неужели\",\n",
    "\"не\",\n",
    "\"ни\",\n",
    "\"так\",\n",
    "\"точно\",\n",
    "\"конечно\",\n",
    "\"едва\",\n",
    "\"только\",\n",
    "\"всего\",\n",
    "\"исключительно\",\n",
    "\"где\",\n",
    "\"куда\",\n",
    "\"откуда\",\n",
    "\"когда\",\n",
    "\"зачем\",\n",
    "\"почему\",\n",
    "\"отчего\",\n",
    "\"как\",\n",
    "\"сколько\",\n",
    "\"насколько\",\n",
    "\"что\",\n",
    "\"кто\",\n",
    "\"какой\",\n",
    "\"каков\",\n",
    "\"который\",\n",
    "\"чей\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26779486",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673bae2",
   "metadata": {},
   "source": [
    "# Часть 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fbea17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Часть 1\n",
    "\n",
    "def load_csv(path: Path, sep: str = \";\") -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    return df\n",
    "\n",
    "def load_txt(path: Path, sep: str = \"SEP\") -> pd.DataFrame:\n",
    "    text = path.read_text()\n",
    "    parts = text.split(sep)\n",
    "    parts = [p.strip() for p in parts if p]\n",
    "    return pd.Series(parts, dtype=\"string\").reset_index(drop=True)\n",
    "\n",
    "def load_docx(path: Path, sep: str = \"SEP\") -> pd.Series:\n",
    "    doc = docx.Document(path)\n",
    "    text = ''.join(paragraph.text for paragraph in doc.paragraphs)\n",
    "    parts = [p.strip() for p in text.split(sep)]\n",
    "    return pd.Series(parts, dtype=\"string\").reset_index(drop=True)\n",
    "\n",
    "def load_pdf(path: Path, sep=\"SEP\") -> pd.Series:\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\".join(page.extract_text() for page in reader.pages)\n",
    "    parts = [p.strip() for p in text.split(sep)]\n",
    "    return pd.Series(parts, dtype=\"string\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "def load(path: Path) -> pd.DataFrame | pd.Series:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    match path.suffix:\n",
    "        case \".csv\":\n",
    "            return load_csv(path)\n",
    "        case \".txt\":\n",
    "            return load_txt(path)\n",
    "        case \".docx\":\n",
    "            return load_docx(path)\n",
    "        case \".pdf\":\n",
    "            return load_pdf(path)\n",
    "        case _:\n",
    "            raise ValueError\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d7a805",
   "metadata": {},
   "source": [
    "# Часть 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97d23c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def _clean(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text, _ = re.subn('[0-9]',\"\", text)\n",
    "    text, _ = re.subn('[,.;:]', '', text)\n",
    "    text, _ = re.subn(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def clean(text_series: pd.Series) -> pd.Series:\n",
    "    result = text_series.copy()\n",
    "    result = result.apply(_clean)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0ec86",
   "metadata": {},
   "source": [
    "# Часть 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c99b81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "stemmer = SnowballStemmer('russian')\n",
    "\n",
    "# russian_stopwords = stopwords.words('russian')\n",
    "all_stopwords = set( additional_stopwords)\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    for item in x:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            yield from flatten(item)\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def _token_word(text):\n",
    "    return word_tokenize(text, language=\"russian\")\n",
    "def _token_sent(text):\n",
    "    return sent_tokenize(text, language=\"russian\")\n",
    "\n",
    "\n",
    "def get_tokens_sent(series: pd.Series) -> pd.Series:\n",
    "    series_parts = flatten([_token_sent(row) for row in series])\n",
    "    return pd.Series(series_parts, dtype=\"string\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_tokens_word(series: pd.Series) -> pd.Series:\n",
    "    series_parts = flatten([_token_word(row) for row in series])\n",
    "    return pd.Series(series_parts, dtype=\"string\").reset_index(drop=True)\n",
    "\n",
    "def stem_text(tokens):\n",
    "    return pd.Series([stemmer.stem(word) for word in tokens], dtype='string').reset_index(drop=True)\n",
    "\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    lemmas = []\n",
    "    for word in tokens:\n",
    "        # Анализ слова и выбор наиболее вероятной формы\n",
    "        parsed = morph.parse(word)[0]\n",
    "        lemmas.append(parsed.normal_form)\n",
    "    return pd.Series(lemmas, dtype=\"string\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def delete_stop(series: pd.Series) -> pd.Series:\n",
    "    result = []\n",
    "    for token in series:\n",
    "        if not token in all_stopwords: \n",
    "            result.append(token)\n",
    "    \n",
    "    return pd.Series(result, dtype=\"string\").reset_index(drop=True)\n",
    "\n",
    "def _tokenise(series: pd.Series) -> pd.Series:\n",
    "    result = get_tokens_word(series)\n",
    "    result = delete_stop(result)\n",
    "    result = stem_text(result)\n",
    "    result = lemmatize_text(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c75d45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "path = \"news_5k.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39997c4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "path = Path(path)\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "serega = load(path)\n",
    "print(serega)\n",
    "sergei = clean(serega)\n",
    "print(sergei)\n",
    "sergei_sergeevich = _tokenise(sergei)\n",
    "print(sergei_sergeevich)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d5308",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def statistics(path):\n",
    "    data = load(path)\n",
    "    data = clean(data)\n",
    "    print(\"-\"*40)\n",
    "    print(data.info())\n",
    "    print(\"-\"*40)\n",
    "    print(data.describe())\n",
    "\n",
    "    tokens_word = get_tokens_word(data)\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    print(tokens_word.info())\n",
    "    print(\"-\"*40)\n",
    "    print(tokens_word.describe())\n",
    "    tokens_word = tokens_word.apply(len)\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    print(tokens_word.info())\n",
    "    print(\"-\"*40)\n",
    "    print(tokens_word.describe())\n",
    "    tokens_sent = get_tokens_sent(data)\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    print(tokens_sent.info())\n",
    "    print(\"-\"*40)\n",
    "    print(tokens_sent.describe())\n",
    "    tokens_sent = tokens_sent.apply(len)\n",
    "\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    print(tokens_sent.info())\n",
    "    print(\"-\"*40)\n",
    "    print(tokens_sent.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd5911",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "p1 = \"news_5k.txt\"\n",
    "p2 = \"news_5k.docx\"\n",
    "p3 = \"news_5k.pdf\"\n",
    "p4 = \"news_5k.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9355de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "statistics(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b152f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "statistics(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060df81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "statistics(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b68921",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "statistics(p4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
